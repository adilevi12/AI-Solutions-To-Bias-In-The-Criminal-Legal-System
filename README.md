# AI-Solutions-To-Bias-In-The-Criminal-Legal-System
Prediction algorithms are increasingly being used in criminal justice. In particular COMPAS software is used in courts across the US. Even though the use of AI is undoubtedly effective and beneficial, it raises many technical, legal, and ethical questions. A reverse model published by ProPublica was made to show that the tool is biased against African Americans in the US.
In this Report, we are going to question how a biased outcome affects human and constitutional rights, especially the defendant's rights. Then, we will discuss how affirmative action might solve the bias through 3 possible methods.
Our main question is, what are the possible solutions to assure fairness (applied in the case study of the ADM made by COMPAS)?
